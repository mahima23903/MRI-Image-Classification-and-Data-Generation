# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qKbkTR2Cmq_dSdjgEGQUXYu79hXxV-vp
"""

import requests
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize
import pandas as pd
import random
import time

nltk.download('punkt')
nltk.download('punkt_tab')

# Define URLs grouped by tumor class
class_links = {
    "glioma": [
        "https://www.ncbi.nlm.nih.gov/books/NBK441874/",
        "https://www.hopkinsmedicine.org/health/conditions-and-diseases/gliomas",
        "https://www.cancerresearchuk.org/about-cancer/brain-tumours/types/glioma-adults"
    ],
    "meningioma": [
        "https://www.ncbi.nlm.nih.gov/books/NBK560538/",
        "https://www.hopkinsmedicine.org/health/conditions-and-diseases/meningioma",
        "https://www.cancer.gov/rare-brain-spine-tumor/tumors/meningioma"
    ],
    "pituitary": [
        "https://www.ncbi.nlm.nih.gov/books/NBK554451/",
        "https://www.hopkinsmedicine.org/health/conditions-and-diseases/pituitary-tumors",
        "https://www.cancer.gov/types/pituitary/patient/pituitary-treatment-pdq"
    ]
}

# Extract 5-sentence chunks from a given URL
def extract_chunks(url, chunk_size=5):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Clean unwanted HTML elements
        for tag in soup(['script', 'style', 'noscript', 'footer', 'header', 'nav']):
            tag.decompose()

        text = soup.get_text(separator=' ')
        text = ' '.join(text.split())  # Normalize whitespace

        # Split into sentences
        sentences = sent_tokenize(text)

        # Form 5-sentence chunks
        chunks = [' '.join(sentences[i:i + chunk_size])
                  for i in range(0, len(sentences) - chunk_size + 1, chunk_size)]

        return chunks

    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return []

# Main data collection loop
def collect_labeled_data():
    data = []
    target_per_class = 500
    collected = {'glioma': 0, 'meningioma': 0, 'pituitary': 0}

    for label, urls in class_links.items():
        print(f"\nCollecting data for: {label.upper()}")
        random.shuffle(urls)
        for url in urls:
            if collected[label] >= target_per_class:
                break

            print(f"ðŸ“¥ Fetching from: {url}")
            chunks = extract_chunks(url)

            for chunk in chunks:
                if len(chunk.split()) < 20:  # Skip very short chunks
                    continue
                if collected[label] >= target_per_class:
                    break
                data.append({'text': chunk, 'label': label})
                collected[label] += 1

            time.sleep(1)  # Be polite

        print(f"Collected {collected[label]} chunks for {label}")

    return data

# Run the collector
dataset = collect_labeled_data()

# Shuffle and save the dataset
random.shuffle(dataset)
df = pd.DataFrame(dataset)
df.to_csv("brain_tumor_dataset_1500.csv", index=False)

print("\n Done! Dataset saved to brain_tumor_dataset_1500.csv")

df = pd.read_csv("brain_tumor_dataset_1500.csv")
df.head()

df.info()