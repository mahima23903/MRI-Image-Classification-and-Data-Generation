# -*- coding: utf-8 -*-
"""Bio-Gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V-Ct9TesfXS1nEkV6t7ROfyr4dEjeg6U
"""

!pip install torch transformers accelerate datasets loralib peft beautifulsoup4 requests

!pip install bitsandbytes sentencepiece

pip install sacremoses pymupdf

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import torch

# Check for GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "microsoft/biogpt-large"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

import pandas as pd

df = pd.read_csv("/content/brain_tumor_dataset_1500.csv")

print(df.head(20))

from transformers import AutoTokenizer

# Define model name
model_name = "microsoft/biogpt"

# Load BioGPT tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Sample medical text dataset (Replace with your data)
medical_texts = df['text'].tolist()

# Tokenize text
inputs = tokenizer(medical_texts, padding=True, truncation=True, return_tensors="pt")

print(inputs)  # Check tokenized output

import torch
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datasets import Dataset

# Load BioGPT model
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# Convert data into Hugging Face Dataset format
dataset = Dataset.from_dict({"text": medical_texts})

# Tokenize dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Define data collator for causal language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Causal LM does not use masked language modeling (MLM)
)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./biogpt_finetuned",
    eval_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=6,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    report_to="none"
)

# Trainer for fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    eval_dataset=tokenized_datasets,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Start fine-tuning
trainer.train()

# Save model
model.save_pretrained("./biogpt_finetuned")
tokenizer.save_pretrained("./biogpt_finetuned")

# Load model for inference
from transformers import pipeline

fine_tuned_biogpt = pipeline("text-generation", model="./biogpt_finetuned", tokenizer=tokenizer)

# Test with a question
input_text = "pituitary"
output = fine_tuned_biogpt(input_text, max_length=500)

print(output)

x = output[0]['generated_text']


import textwrap
print("\n".join(textwrap.wrap(x, width=100)))

